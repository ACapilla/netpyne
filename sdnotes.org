* 15may27 Modularized framework for NEURON/Python network simulations with MPI
** Overview
Using this modularized structure can select different cell types, populations, connectivities, etc. just by modifying the
parameters in params.py. 

An example is provided in params.py which switches from a single Hodgkin-Huxley population, randomly connected model
(mpiHHTut); to a multiple Izhikevich populations, yfrac-dependent connected M1 model.

To test this just modify the variable simType in params.py: 
simType = 'M1model' # 'mpiHHTut' 

*** Bill's description of what this repo should be
we should try to make this our main organizational code that can spin off multiple projects? -- would like a clean code center-org so that not so painful as it has been with our prior code base (which btw was all my fault and not sam's -- he inherited from me)
then any pushes back to this main center would have to be agreed by curator (you) and would generally involve just some small fix to a single file

** README with description of the different files
List of files:

- main.py: Main executable; calls functions from other modules.

- params.py: Contains all static parameters. It is imported as "p" from all other modules, so that params can be referenced from any file using p.paramName

- shared.py: Contains all the model shared variables and modules (except params.py). It is imported as "s" from all other file, so that any variable or module can be referenced from any file using s.varName

- sim.py: Simulation control functions (eg. runSim)

- network.py: Network related functions (eg. createCells)

- cell.py: contains cell and population classes 

- conn.py: contains class 'Conn' used to instantiate connections, and contains methods to connect cells (eg. random, yfrac-based)

- analysis.py: functions to plot and analyse data

- stimuli.py: functions and parameters for differnt types of neural stimulation

- izhi2007.mod: NMODL definition of Izhikevich 2007 neuron model

- evol.py: Functions to run evolutionary algorithms (using inspyred) to optimize model parameters

** Known issues, potential improvements, and alternative implementations
*** TODO - Saving only happens at the end (instead of at fixed intervals)
*** TODO - Need to add a DEBUG mode or similar (like mpiHHTut had)
- currently have a verbose flag, that prints some additional info
- also have print messages along the code, which could be made optional
*** TODO - Check structure of data saved to file
- some nested lists with inhomogeneous dimensionality were converted to 'object' data types in order to be saved -- check if
  structure makes sense when reading from python/matlab
*** DONE - Some attributes/variables of the class 'Cell' and 'Pop' are not used but still initialized to [ ]
- maybe can find alternative using different constructors for different cell types
- fixed by using dictionary of tags - flexible for eahc type
*** DONE - Labels (eg. AMPA, IT, HH,...) are defined by assigning numeric values to variables
- can use dictionaries instead
- need to check pros and cons of each -- are dicts slower or have any limitations compared to lists/arrays ?
- changed to using dictionaries
*** TODO - stimuli.py not tested with this model
- left there cause thought could be useful for future
*** TODO - analysis.py still has many functions which havent been adapted to new structure
*** TODO - Some variables inherited from cliff still have too long names, eg. backgroundSpikevec
*** TODO - Lines too long, need to split using \
*** DONE - if increase p.backgroundNoise > 0.0 get memory error 
- had to store rnadom noise generator separately for each cell
*** DONE - Find a way to save lambda funcs (can't be pickled)
can use:
>>>import inspect 
>>>inspect.getsource(myfunc)
*** DONE - Clearing vector in simdata now gives error because also has python variables
for v in s.simdata.itervalues() -- fix error
separated vectors
*** TODO - 'Cell' class record method uses eval() which is unsafe
*** DONE - currently setup to record traces from all cells
*** DONE - bug when saving cell traces using multiple cores 
Gathering spikes...
>>> >>> >>> Traceback (most recent call last):
  File "main.py", line 46, in <module>
    runSeq()
  File "main.py", line 37, in runSeq
    s.sim.gatherData()
  File "sim.py", line 117, in gatherData
    for d in gather: tmp.extend(d[k]) 
KeyError: 'cellTraces_233'

- now saved using different structure 
- had to remove hoc objects that are not pickable


*** TODO - Load/save net+sim parameters from file
- load net and sim parameters file from mat?
- have different .py file to generate the params files

within params.py - have option to load from file, or set params and save to 2 files net and sim.mat
- maybe functions in sim.py? can be called from params?
*** DONE - Store net and sim params in dictionaries inside params.py
eg. p.net['ncells']
- facilitates saving to file (prev point)
- can use if key in dict:

- use regexp in submlime text to replace in all files:
Find What: p\.(\w+)
Replace With: p\.sim['$1']

*** TODO - Use dict of tags/attributes for pop params and for cells
eg. p.net['popParams'][i]['cellModel']
- can use if key in dict:
*** Define conn rules based on pair of tag/value for pre and post
eg. use tuple key: connProbs['name','IT','name','IT']  = (lambda x,y: 0.1*x+0.01/y) 
or dict of dict with tuple keys: connProbs['name','IT']['name','IT']  = (lambda x,y: 0.1*x+0.01/y) 
*** DONE - Add $Id$ hg info
- can do with https://mercurial.selenic.com/wiki/KeywordExtension
- but not recommended 
- bill wanted to do because cliff and I add last update and author info to files -- which is never updated correctly
- checked other github repos and they don't have it - just eg. Contributors
*** DONE - Rename main.py with init.py
*** DONE Remove popType and connType variables - infer from dict keys
*** DONE Remove popid from popParams - can use list index
*** TODO Replace topClass and subClass with projection-type, type (neurotransmitter involved?)
**** email to Ben
Hey Ben, Im working on some of the changes we discussed. I've replaced variables with dictionaries of tags/attributes. For now, I've kep the 'population' concept, although can replace in future version if makes sense. 

For both the 'population' and 'cell' objects you suggested replacing the 'topClass' and 'subClass' tags with 'projectionType' and 'cellType' if my notes are correct. I know projType for Exc cells will be 'IT', 'PT' or 'CT', but not sure what would be the best classification for Inh cells? Same thing for cellType, I think you mentioned neurotransmitters involved, but could you elaborate on what would be the list of possible values for both 'Exc' and 'Inh' cells/pops ?  

We can use the google chat or this google doc to bounce ideas back and forth (link points to new section ready to be filled in).

*** TODO Synapse
- synapses as list of objects inside each cell (postsynaptic) - netcon in pre is stub; netcon in post is real synapse
- netcon (neuron object) as part of synapse object


* 15aug18 reviews Subject: NECO-03-15-2319 Reviewer Comments
** Dear Dr. Lytton,
     We have received comments on your manuscript "Simulation neurotechnologies for advancing brain research: Parallelizing
large networks in NEURON" submitted to Neural Computation.  It cannot be accepted in its current form, but a revised version
that addresses the concerns of the reviewer(s) could be considered if we receive your resubmission within three months.
     If you would like to resubmit the paper, please revise it according to the editor(s) comments and include a cover letter
that specifically outlines the changes you have made, point by point.
     When you are ready to upload your revisions, please log in and follow the links for this manuscript.
http://neuralcomp.allentrack.net/cgi-bin/main.plex?el=A2Bq5CEC3A1aE6I7A9bO3MdMJMHrcuDCA9yinLAZ
** Reviewer #1 (Comments to the Authors):
This paper points out the importance that tools for the large scale parallel simulation of realistic neuronal network models
hold for understanding the brain. This description of their implementation within the NEURON simulator is an important,
original, and significant contribution, It should have a high priority for publication. I have no objections to publishing
the paper as submitted. However, I offer the following suggestions in the event that the authors plan on further revisions.

The Introduction suggests that the following sections would give the reader an overview of how a simple network with
excitatory connections would be implemented in parallel NEURON. This overview would be useful to many readers who are not
modeling networks with NEURON. But, I found no mention of the model in the remainder of the paper.  (The Hines 2014 reference
was to a description of modelDB. not the model, and the model was not yet available on modelDB.) The Methods section would
have been a good place to very briefly describe the model and its connection scheme. Details could be given in the following
paper. Then the following descriptions of the use of ParallelContext and NetCon could be given in the context of implementing
this model. Without this overview, I found the description of using NetCon in a parallel context to be very abstract, and
hard to relate to an actual model. Perhaps some short script fragments with explanations would clarify this, I believe that
this would make the paper much more comprehensible to a more general reader who is not a NEURON user. For such a reader, this
paper will be a difficult read.

it would be helpful to mention the hardware and software requirements for using NEURON in a parallel MPI context. For example,
is the implementation of MPI built in to NEURON, or is a particular open source package such as openmpi or MPICH required?

The topics that were covered under Results were treated thoroughly and in a way that clearly exposed the
simulator-independent issues encountered in parallelizing a network simulation. The NEURON-specific examples provided the
details.

In Sec 3.4, I was baffled by the expression "an implicit pickle", until I read the description of Python's "pickle" module in
a following section. Some reorganization would make this section more accessible to non-experts.

In general, this is an informative and useful paper. With some (optional) revision, it could be made more appealing to a
reader with a more general background in neural modeling.

** Reviewer #2 (Comments to the Authors):

Simulation neurotechnologies for advancing brain research: Parallelizing large networks in NEURON Lytton et al.

This paper presents a broad overview of the use of the NEURON simulator, with an emphasis on parallel modeling. While NEURON
is an important part of current neuronal modeling efforts and advances in its functionality are of interest to many groups,
this paper does not effectively present these advances. The paper makes many general points about simulator methodology,
with a few NEURON-specific illustrations, but does not present measurements, benchmarks, or quantifiable comparisons. In
many cases it is difficult to discern how much of the presented material is novel to NEURON, novel to the field, or just an
illustration of how to accomplish certain tasks using the Python interface.

If the goal of this paper is to present the recent advances in parallel Neuron, it needs to clearly specify what the advances
are, and what they accomplish.

The paper also gives the sense of discussing many points in terms of experience with parallel models. Unfortunately it does
so through generalities and examples, rather than as a data-driven exercise. There are no graphs or numerical results to
make the points. 

Major points.
1. The authors state that they "present an update to our prior work on .. using parallel computing..."
However the introduction does not clearly indicate what is the prior work, nor what the update consists of.

2. The first part of the Methods section reads somewhat like an instruction manual, or even a primer at points. What it
doesn't convey is how one sets up the example simulation. There are no simulation results shown, nor any benchmarks.

3. Similarly section 2.1 is a somewhat unstructured listing of Python capabilities, without a clear motivation to explain
why we are learning these things.

4. In section 3.1, the discussion of spike exchange suffers from lack of figures and lack of quantitative measures
explaining how each solution performs. There are several statements (for example, regarding Multisend and Allgather) which
need to be supported with data. 

5. Similarly section 3.2 repeats previous findings on fixed vs variable timestep methods. It isn't clear, for example,
whether the discussion on the hazards of variable timestep methods in large network simulations, is a recap or new results.
If the latter, then we need data and preferably illustrations. 

6. Later in section 3.2 (last half of page 17, 18) there is a discussion and advice on how one may partition computations in
a hybrid network model. Nevertheless it is difficult to discern the source of the results upon which this discussion is
based. 

7. Section 3.3 brings up "multiple permutations for a complete data handling scheme" and suggests idioms to handle these. It discusses
in-memory storage and continuous dumping to disk, and various ways to organize these. It is not clear which of this, if any, is novel.
Some of it seems to have been done in earlier NEURON work, and much appears to be part of the general consensus in the field
about parallel data storage, of any kind. The authors relate what Neuron does for data saving, but do not attempt to
distinguish it from capabilities of earlier versions, or from other approaches to data storage.

8. A similar concern about novelty applies to section 3.4, sending data to the master node. The authors relate the usage of
a python call, py_alltoall. At face value, these calls don't seem very different from a generic parallel application.
Further, I don't see any results on performance here. 

9. The data formats section is similarly general, lacking quantification. In this case the authors discuss several optional
methods but don't benchmark any of them. None of these steps seem to go much further than the standard Python interfaces for
data formatting. 

10. The authors do discuss cell re-simulation and this, in principle, could be interesting. However, they again fail to
benchmark or quantify the trade-offs and when it becomes viable to carry out re-simulation.

11. The section on multisaving (3.7) does begin with some specifics about the use of this technique. Without benchmarks I
    cannot evaluate the efficiency.

** TODO for paper review 
*** R1. Include model + conn in Methods 
*** R2. 1) Prior work intro
*** R2. 2) Example simulation with results and benchmark
- Alex's poster with setup time, run time, and raster plots - for num nodes + num cells in NSG
- Include comparison of Izhi vs HH
*** R2. 3) Re-structure methods to make clear why using Python
*** R2. 4) same as 2)
*** R2. 5) Load-balancing - move to methods? perform comparison of fixed vs variable?
*** R2. 6) Hybrid networks - either a) move to discussion or to b) Methods + run small sim
*** R2. 7) Saving - move to Methods, describe several options (pickle, mat, txt, etc)
*** R2. 8) Gathering data in master node - move to Methods (sim.py)
- maybe add post-simulation benchmark time (gathering, etc)
*** R2. 9) Move to saving section, with brief description of formats; possibly include comparison of file sizes + saving time
- relate to NSG data format
*** R2. 10) Re-simulation - consider if relevant for this paper?
*** R2. 11) Multisaving - maybe mention briefly in Methods saving section

** New paper structure
*** Intro
*** Methods
**** NEURON/Python basic commands
**** NEURON/Python mpi/parallel commands
**** Example of code structure/simulation with simple network example
- this is a proposal for a general python/NEURON framework to develop mpi/parallel models
- figure with code structure - modularity, shared variables and params
- params.py illustrate how to change
- shared.py
- network.py - net functions
- sim.py - saving, gather
- analysis.py - graphing
- cell.py 
- conn.py

- random num generators 
- load balancing??
**** NSG Usage
- how to run

*** Results
- raster plots
- conn matrix 
- Izhi vs HH; setup vs runtime vs after-run times (gathering etc) (Num nodes and num cells in NSG) + avg
- Compare performance for different num of segments

*** Conclusions

** Alex TODO list
- clone to neurosim
- add stopwatch to sim.py
- test in NSG
- implement HH vs Izhi network
- Run sims for paper
- Write paper

** Bill old list of things to do 
*** this paper does not effectively present these advances. The paper makes many general points about simulator methodology,
with a few NEURON-specific illustrations
*** does not present measurements, benchmarks, or quantifiable comparisons. In many cases it is difficult to discern how much
of the presented material is novel to NEURON, novel to the field, or just an illustration of how to accomplish certain tasks
using the Python interface.
*** If the goal of this paper is to present the recent advances in parallel Neuron, it needs to clearly specify what the
advances are, and what they accomplish.
*** The paper also gives the sense of discussing many points in terms of experience with parallel models. Unfortunately it does
so through generalities and examples, rather than as a data-driven exercise. There are no graphs or numerical results to
make the points. 
*** Major points.
*** introduction does not clearly indicate what is the prior work, nor what the update consists of
1. The authors state that they "present an update to our prior work on .. using parallel computing..."

*** methods does not convey is how one sets up the example simulation. There are no simulation results shown, nor any benchmarks.
2. The first part of the Methods section reads somewhat like an instruction manual, or even a primer at points. What it
doesn't 

*** sec 2.1 is somewhat unstructured listing of Python capabilities, without a clear motivation to explain
why we are learning these things.

*** several statements (for example, regarding Multisend and Allgather) which need to be supported with data
In section 3.1, the discussion of spike exchange suffers from lack of figures and lack of quantitative measures
explaining how each solution performs. There are 

*** hazards of variable timestep methods in large network simulations -- a recap or new results??
5. Similarly section 3.2 repeats previous findings on fixed vs variable timestep methods. It isn't clear, for example,
whether the discussion on the ; If the latter, then we need data and preferably illustrations. 

6. how one may partition computations in a hybrid network model
 Later in section 3.2 (last half of page 17, 18) there is a discussion and advice on how one may partition computations in
a hybrid network model. Nevertheless it is difficult to discern the source of the results upon which this discussion is
based. 

*** data saving -- It is not clear which of this, if any, is novel.
7. Section 3.3 brings up "multiple permutations for a complete data handling scheme" and suggests idioms to handle these. It discusses
in-memory storage and continuous dumping to disk, and various ways to organize these. It is not clear which of this, if any, is novel.
Some of it seems to have been done in earlier NEURON work, and much appears to be part of the general consensus in the field
about parallel data storage, of any kind. The authors relate what Neuron does for data saving, but do not attempt to
distinguish it from capabilities of earlier versions, or from other approaches to data storage.

*** novelty? section 3.4, sending data to the master node. The authors relate the usage of
8. A similar concern about novelty applies to section 3.4, sending data to the master node. The authors relate the usage of
a python call, py_alltoall. At face value, these calls don't seem very different from a generic parallel application.
Further, I don't see any results on performance here. 

*** data formats section -- benchmarking
9. The data formats section is similarly general, lacking quantification. In this case the authors discuss several optional
methods but don't benchmark any of them. None of these steps seem to go much further than the standard Python interfaces for
data formatting. 

*** re-sim benchmarking
10. The authors do discuss cell re-simulation and this, in principle, could be interesting. However, they again fail to
benchmark or quantify the trade-offs and when it becomes viable to carry out re-simulation.

*** multisaving -- benchmarking
11. The section on multisaving (3.7) does begin with some specifics about the use of this technique. Without benchmarks I cannot evaluate the efficiency.


* Claustrum net - George Augustine and Jing
** initial chat with Jing to implement network

salvadord [12:10 PM] 
hi jing, yeah I can help you with the network stuff -- where is the current repo so I can check out the code? what info do we have about network? I discussed with george augustine and he said you had 'unitary conn strenghts' but not enough info for 'probability of connection' between populations

jing [12:14 PM] 
Hi @salvadord, thanks for agreeing to help

​[12:16] 
yes, I guess so. we have data regarding the level of excitation in the presynaptic cell and the consequent level of activation of the postsynaptic cell

​[12:17] 
we might be able to approx the probability of connection since also know how many presynaptic cell  excites a postsynaptic cell

salvadord [12:18 PM] 
ok sounds good... so how many populations do you have? do u have approx cell numbers or densities?

jing [12:18 PM] 
the code doens't work at the moment because I'm trying to rewrite parts of it -- /u/jing/claus

​[12:18] 
approx cell numbers or densities -- we have that data but it's not with me

billl [12:19 PM] 
7 types I think

jing [12:19 PM] 
cell types 7 yes

​[12:19] 
currently modelled 6

​[12:19] 
the 7 types are P, SA1, SA2, SA3, SA4, MA1, MA2

salvadord [12:20 PM] ok cool... so let me update the network framework with the last changes and you can clone from there --
I can go over a simple example with you and help you fill in a new parameters file for this network

salvadord [1:07 PM] 
ok sounds good, I'll put some info in this channel and we can discuss tonite

salvadord [4:09 PM] jing, you can now clone the python network framework from /usr/site/nrniv/local/sim/simsetup/ -- eg. hg
clone /usr/site/nrniv/local/sim/simsetup /u/jing/clausnet

​[4:10] 
there is a README file that describes what the purpose of the framework is and the main files involved

​[4:11] the general idea is that it facilitates building networks in NEURON by allowing to specify the full model parameters
(populations, cell properties, connectivity and simulation options) in a single file

​[4:12] once all this params are specified you can easily run the network in parallel using mpi, save to different formats and
plots several graphs (raster plot of spikes, LFP, connectivity matrices etc)

​[4:13] The file to specify the params uses a custom format based solely on python dictionaries (so no NEURON code is used
there) -- you can see examples inside the /params folder

​[4:15] eg. /params/mpiHHTut.py implements a simple tutorial model with HH cells; /params/mpiHybridTut.py includes both HH and
Izhikevich cells; and /params/M1yfrac.py implements a more complex M1 cortex network with Izhi cells

​[4:16] the framework makes it very easy to swap different cell types (eg. HH vs Izhi), and to define connectivity rules
between different populations etc

​[4:16] once you have cloned the repo, I can help you set up the param file for the claustrum network, with your specific
populations, cell types, connectivity etc

​[4:17] I have also added import functions so you can read cell definitions from HOC templates or other Python files -- so you
can use the cell models you have already developed
** Next steps to include cell model in network
- copy or link new mod file clausIzhi2007b.mod
- define new cell class in cell.py, or extend current Izhi2007b - if can run old model
- define cell properties, pops adn conn

- ln -s /u/jing/claus/clausIzhi2007b.mod 
- ./compile


* 15dec28 Convert into python package
- PyNet ? NeuPyNE, NeuPyNet, netpyne !! PYthon-based NETwork development framework for the NEuron simulator
- make shared -> framework 
- from pynet import framework as f
- just need init.py and param.py file
- to add cells or conn functions use:
-- class newCellClass(PointNeuron): ... ;  f.newCellClass = newCellClass 
-- class newConnFunc(): ... ; f.newConnFunc = newConnFunc
- default simConfig
- from pynet import init ; init.createAndRun(netParams, simConfig)
