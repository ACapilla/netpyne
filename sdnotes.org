* 15may27 Modularized framework for NEURON/Python network simulations with MPI
** Overview
Using this modularized structure can select different cell types, populations, connectivities, etc. just by modifying the
parameters in params.py. 

An example is provided in params.py which switches from a single Hodgkin-Huxley population, randomly connected model
(mpiHHTut); to a multiple Izhikevich populations, yfrac-dependent connected M1 model.

To test this just modify the variable simType in params.py: 
simType = 'M1model' # 'mpiHHTut' 

*** Bill's description of what this repo should be
we should try to make this our main organizational code that can spin off multiple projects? -- would like a clean code center-org so that not so painful as it has been with our prior code base (which btw was all my fault and not sam's -- he inherited from me)
then any pushes back to this main center would have to be agreed by curator (you) and would generally involve just some small fix to a single file

** README with description of the different files
List of files:

- main.py: Main executable; calls functions from other modules.

- params.py: Contains all static parameters. It is imported as "p" from all other modules, so that params can be referenced from any file using p.paramName

- shared.py: Contains all the model shared variables and modules (except params.py). It is imported as "s" from all other file, so that any variable or module can be referenced from any file using s.varName

- sim.py: Simulation control functions (eg. runSim)

- network.py: Network related functions (eg. createCells)

- cell.py: contains cell and population classes 

- conn.py: contains class 'Conn' used to instantiate connections, and contains methods to connect cells (eg. random, yfrac-based)

- analysis.py: functions to plot and analyse data

- stimuli.py: functions and parameters for differnt types of neural stimulation

- izhi2007.mod: NMODL definition of Izhikevich 2007 neuron model

- evol.py: Functions to run evolutionary algorithms (using inspyred) to optimize model parameters

** Known issues, potential improvements, and alternative implementations (outdated - see TODO list at end of file)
*** TODO - Saving only happens at the end (instead of at fixed intervals)
*** TODO - Need to add a DEBUG mode or similar (like mpiHHTut had)
- currently have a verbose flag, that prints some additional info
- also have print messages along the code, which could be made optional
*** TODO - Check structure of data saved to file
- some nested lists with inhomogeneous dimensionality were converted to 'object' data types in order to be saved -- check if
  structure makes sense when reading from python/matlab
*** DONE - Some attributes/variables of the class 'Cell' and 'Pop' are not used but still initialized to [ ]
- maybe can find alternative using different constructors for different cell types
- fixed by using dictionary of tags - flexible for eahc type
*** DONE - Labels (eg. AMPA, IT, HH,...) are defined by assigning numeric values to variables
- can use dictionaries instead
- need to check pros and cons of each -- are dicts slower or have any limitations compared to lists/arrays ?
- changed to using dictionaries
*** TODO - stimuli.py not tested with this model
- left there cause thought could be useful for future
*** TODO - analysis.py still has many functions which havent been adapted to new structure
*** TODO - Some variables inherited from cliff still have too long names, eg. backgroundSpikevec
*** TODO - Lines too long, need to split using \
*** DONE - if increase p.backgroundNoise > 0.0 get memory error 
- had to store rnadom noise generator separately for each cell
*** DONE - Find a way to save lambda funcs (can't be pickled)
can use:
>>>import inspect 
>>>inspect.getsource(myfunc)
*** DONE - Clearing vector in simdata now gives error because also has python variables
for v in s.simdata.itervalues() -- fix error
separated vectors
*** TODO - 'Cell' class record method uses eval() which is unsafe
*** DONE - currently setup to record traces from all cells
*** DONE - bug when saving cell traces using multiple cores 
Gathering spikes...
>>> >>> >>> Traceback (most recent call last):
  File "main.py", line 46, in <module>
    runSeq()
  File "main.py", line 37, in runSeq
    s.sim.gatherData()
  File "sim.py", line 117, in gatherData
    for d in gather: tmp.extend(d[k]) 
KeyError: 'cellTraces_233'

- now saved using different structure 
- had to remove hoc objects that are not pickable


*** TODO - Load/save net+sim parameters from file
- load net and sim parameters file from mat?
- have different .py file to generate the params files

within params.py - have option to load from file, or set params and save to 2 files net and sim.mat
- maybe functions in sim.py? can be called from params?
*** DONE - Store net and sim params in dictionaries inside params.py
eg. p.net['ncells']
- facilitates saving to file (prev point)
- can use if key in dict:

- use regexp in submlime text to replace in all files:
Find What: p\.(\w+)
Replace With: p\.sim['$1']

*** TODO - Use dict of tags/attributes for pop params and for cells
eg. p.net['popParams'][i]['cellModel']
- can use if key in dict:
*** Define conn rules based on pair of tag/value for pre and post
eg. use tuple key: connProbs['name','IT','name','IT']  = (lambda x,y: 0.1*x+0.01/y) 
or dict of dict with tuple keys: connProbs['name','IT']['name','IT']  = (lambda x,y: 0.1*x+0.01/y) 
*** DONE - Add $Id$ hg info
- can do with https://mercurial.selenic.com/wiki/KeywordExtension
- but not recommended 
- bill wanted to do because cliff and I add last update and author info to files -- which is never updated correctly
- checked other github repos and they don't have it - just eg. Contributors
*** DONE - Rename main.py with init.py
*** DONE Remove popType and connType variables - infer from dict keys
*** DONE Remove popid from popParams - can use list index
*** TODO Replace topClass and subClass with projection-type, type (neurotransmitter involved?)
**** email to Ben
Hey Ben, Im working on some of the changes we discussed. I've replaced variables with dictionaries of tags/attributes. For now, I've kep the 'population' concept, although can replace in future version if makes sense. 

For both the 'population' and 'cell' objects you suggested replacing the 'topClass' and 'subClass' tags with 'projectionType' and 'cellType' if my notes are correct. I know projType for Exc cells will be 'IT', 'PT' or 'CT', but not sure what would be the best classification for Inh cells? Same thing for cellType, I think you mentioned neurotransmitters involved, but could you elaborate on what would be the list of possible values for both 'Exc' and 'Inh' cells/pops ?  

We can use the google chat or this google doc to bounce ideas back and forth (link points to new section ready to be filled in).

*** TODO Synapse
- synapses as list of objects inside each cell (postsynaptic) - netcon in pre is stub; netcon in post is real synapse
- netcon (neuron object) as part of synapse object



* 15dec28 Convert into python package
- PyNet ? NeuPyNE, NeuPyNet, netpyne !! PYthon-based NETwork development framework for the NEuron simulator
- make shared -> framework 
- from pynet import framework as f
- just need init.py and param.py file
- to add cells or conn functions use:
-- class newCellClass(PointNeuron): ... ;  f.newCellClass = newCellClass 
-- class newConnFunc(): ... ; f.newConnFunc = newConnFunc
- default simConfig
- from pynet import init ; init.createAndRun(netParams, simConfig)

* 16jan26 Possibly making more simulator-indep using NeuroML-based format
** NeuroML format 
- could represent all nml format using python dicts (instantiated net)
- additional abstract layer (connParams, netParams etc) - using more general format and then converted to nml
- eg. https://github.com/OpenSourceBrain/Thalamocortical/tree/master/neuroConstruct/generatedNeuroML2
- https://www.neuroml.org/getneuroml
- https://www.neuroml.org/tool_support
- https://github.com/NeuralEnsemble/libNeuroML
- https://github.com/NeuroML/pyNeuroML
- http://bioportal.bioontology.org/ontologies/CNO
- http://www.neuroconstruct.org/
- http://neuronvisio.org/


*** Issues
- Would like to keep simple declarative (python dicts based) specifications
- Want to make netpyne more NEURON-independent, both at specifications, and py instantiated network
- netpyne makes use of mod files, but neuroml requires detailed specification
- netpyne provides conversion from abstract specifications -> instantiated network, and support for multicompartment, and
  subcellular connectivity
- netpyne provides instantiation of NEURON objects, and parallel simulation of network
- netpyne can use NeuroML structure but using dictionaries instead of xml text - so easy to manipulate
- libneuroml and pynn - procedural, not so intuitive, and require internal definition of cell types etc (eg. IAF); whereas
  netpyne is 'declarative', more intuitive?, can define arbitrary cell props based on mechs and syns
- dictionary of equivalences NEURON<->NeuroML (eg. tau1, rise)

*** cells
NOTE: segment = sections/segment; segmentGroup = sectionList/section
- can have segmentGroups that consist of segmentGroups
- Neuron sections->segmentGroups
- Neuron 3d points -> segmnets
- Neuron nseg -> property tag numberInternalDivisions
- if have pointProcess in segment -> mapping of position along section and where pointprocess is placed

    <include href="nap.channel.nml"/>

    <include href="pas.channel.nml"/>

<cell id="SupAxAx">

        <notes>Cell: supaxax_0 exported from NEURON ModelView</notes>

        <morphology id="morphology_SupAxAx">

            <segment id="0" name="Seg0_comp_1">
                <proximal x="0.0" y="0.0" z="0.0" diameter="15.0"/>
                <distal x="0.0" y="10.0" z="0.0" diameter="15.0"/>
            </segment>

            <segment id="1" name="Seg1_comp_1">
                <parent segment="0"/>
                <distal x="-4.371139E-7" y="20.0" z="0.0" diameter="15.0"/>
            </segment>

 <segmentGroup id="prox_dend_soma">
                <include segmentGroup="comp_1"/>
                <include segmentGroup="comp_41"/>
                <include segmentGroup="comp_28"/>
                <include segmentGroup="comp_15"/>
                <include segmentGroup="comp_2"/>
            </segmentGroup>


	<biophysicalProperties id="biophys">

            <membraneProperties>
                
                <channelDensity condDensity="0.0 mS_per_cm2" id="ar_ModelViewParmSubset_1" ionChannel="ar__m00_25" segmentGroup="ModelViewParmSubset_1" erev="-40.0 mV" ion="ar"/>
                
                <channelDensity condDensity="0.1 mS_per_cm2" id="cal_ModelViewParmSubset_4" ionChannel="cal" segmentGroup="ModelViewParmSubset_4" ion="ca" erev="125.0 mV"/>
                
                <channelDensity condDensity="0.2 mS_per_cm2" id="cal_ModelViewParmSubset_5" ionChannel="cal" segmentGroup="ModelViewParmSubset_5" ion="ca" erev="125.0 mV"/>
                
                <channelDensity condDensity="0.0 mS_per_cm2" id="cat_ModelViewParmSubset_1" ionChannel="cat" segmentGroup="ModelViewParmSubset_1" ion="cat" erev="125.0 mV"/>
                
                <channelDensity condDensity="0.0 mS_per_cm2" id="k2_all" ionChannel="k2" ion="k" erev="-100.0 mV"/>
                
                <spikeThresh value="0.0 mV"/>

                <specificCapacitance value="1.0 uF_per_cm2"/>

                <initMembPotential value="-65.0 mV"/>


*** connections
<projection id="LTS_AxAx_sm" presynapticPopulation="CG_C04_LTS_sm" postsynapticPopulation="CG_C04_AxAx_sm" synapse="Inh_LTS_FS">
            <connection id="0" preCellId="../CG_C04_LTS_sm/5/SupLTSInter" postCellId="../CG_C04_AxAx_sm/0/SupAxAx" preSegmentId="112" preFractionAlong="0.26785243" postSegmentId="82" postFractionAlong="0.36041966"/>
            <connection id="1" preCellId="../CG_C04_LTS_sm/3/SupLTSInter" postCellId="../CG_C04_AxAx_sm/0/SupAxAx" preSegmentId="116" preFractionAlong="0.6782849" postSegmentId="91" postFractionAlong="0.99472666"/>
            <connection id="2" preCellId="../CG_C04_LTS_sm/6/SupLTSInter" postCellId="../CG_C04_AxAx_sm/0/SupAxAx" preSegmentId="112" preFractionAlong="0.42696908" postSegmentId="84" postFractionAlong="0.46009916"/>
            <connection id="3" preCellId="../CG_C04_LTS_sm/0/SupLTSInter" postCellId="../CG_C04_AxAx_sm/0/SupAxAx" preSegmentId="113" preFractionAlong="0.92599744" postSegmentId="83" postFractionAlong="0.60695267"/>
            <connection id="4" preCellId="../CG_C04_LTS_sm/9/SupLTSInter" postCellId="../CG_C04_AxAx_sm/0/SupAxAx" preSegmentId="113" preFractionAlong="0.5823235" postSegmentId="39" postFractionAlong="0.646692"/>
            <connection id="5" preCellId="../CG_C04_LTS_sm/6/SupLTSInter" postCellId="../CG_C04_AxAx_sm/0/SupAxAx" preSegmentId="113" preFractionAlong="0.16917303" postSegmentId="57" postFractionAlong="0.6089958"/>

*** synapses
<?xml version="1.0" encoding="ISO-8859-1"?>
<neuroml xmlns="http://www.neuroml.org/schema/neuroml2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.neuroml.org/schema/neuroml2 https://raw.github.com/NeuroML/NeuroML2/development/Schemas/NeuroML2/NeuroML_v2beta4.xsd" id="Syn_AMPA_L4SS_IN">

    <notes>ChannelML file describing a single synaptic mechanism</notes>

    <alphaSynapse id="Syn_AMPA_L4SS_IN" tau="0.8ms" gbase="2.94303552937e-07mS" erev="0.0mV">

        <notes>Synapse with syn scaling constant c = 1 nS (translating to max cond of 2.94304e-07 mS), time course: 0.8 ms and reversal potential: 0 mV.
        Automatically generated by command:  genSyn.py Syn_AMPA_L4SS_IN 0.8 1 0 </notes>
</alphaSynapse>

</neuroml>
*** population

        <population id="CG_C04_FRB_sm" component="L23PyrFRB_varInit" type="populationList" size="6">
           <annotation>
                <property tag="color" value="0.0 0.0 0.0"/>
            </annotation>
            <instance id="0">
                <location x="-739.27563" y="-798.43396" z="-440.9723"/>
            </instance>
            <instance id="1">
                <location x="975.8342" y="-870.85077" z="1090.1094"/>
            </instance>
            <instance id="2">
                <location x="-1012.162" y="-785.71173" z="1053.8684"/>
            </instance>
            <instance id="3">
                <location x="-1391.423" y="-776.4188" z="-32.483643"/>
            </instance>
            <instance id="4">
                <location x="149.5564" y="-790.7691" z="433.74915"/>
            </instance>
            <instance id="5">
                <location x="585.53955" y="-765.37933" z="819.1377"/>
            </instance>
        </population>


*** pulse generator (netstim?)
   <pulseGenerator id="DepCurr_L23FRB" delay="0.0s" duration="20.0s" amplitude="3.0E-10A"/>
    
    <pulseGenerator id="DepCurr_L23FRB__0" delay="0.0s" duration="20.0s" amplitude="3.37936E-10A"/>
** chat with Bill
summary of 3-hr meeting with Ben: 1) he really likes netpyne, 2) discussed how to add subcellular syn info (3 main methods: sectionlists, yfrac-based, path distance-based), 3) provided his unpublished code to convert sCRACM data to synapse density maps and said we can use in netpyne, 4) suggested making netpyne more NEURON/simulator-independent (similar to Kim Blackwell)

on this last point (simu-indep), I looked at NeuroML2 format, and some of the tools Padraig has built 2 interface it with Python and Neuron (eg. neuroconstruct, pyNeuroML, libNeuroML), and read Robert's reproducibility paper

I think it might be possible to make netpyne simulator-independent at the specs and instantiated network, using a NeuroML-like internal format (but using python dicts instead of xml so can manipulate data) -- and then keep final stage where its converted to Neuron simulatable objects

sent padraig an email to discuss the technical details of what they've already implemented and how netpyne could fit, and be useful to fill in missing tools in this whole story

an alternative is to keep it as a tool specific for Neuron, with its internal Neuron-based format for specs and network, and the possibility of exporting/importing from NeuroML and others (ie. its current status)

** chat with Padraig
- good to separate M1 model from netpyne
- similarities with pnn and libneuroml
- important of neuroml is conceptual structure
- libneuroml has cell types
- mapping from pyneuroml to mod files, but not the other way around

- standardization of abstract conn rules is above pynn and nueroml/pyneuroml
- eg 10 conn algorithms -> pynn -> neuroml
- morphology - segments, segment groups

- mod files would need to be translated into nueroml (not fully translated); alternative would be point to nueroml
  definition, convert via pyneuroml to Neuron mod
- setup.py to netpyne
- dot.travis.yml continuous integration server - run tests every time there is a commit to repository - make sure its consistent
  with latest version

- convert from Neuron model to NeuroML:
-- export specifications 
-- OSB converting to NeuroML2 cell morphology (pyneuroml.convert_to_neuroml2) - in future if already in neuroml dont need to
 define in Nueron/netpyne format 
-- Izhikevich (params) - abstract definiton
-- export to Nueroml2 positions and connectivity - no current tool to convert
-- https://github.com/OpenSourceBrain/Cerebellum3DDemo
-- http://www.opensourcebrain.org/docs#Converting_To_NeuroML2


- convert from NeuromML to Neuron:
-- java script to convert neuroml to mod and Neuron/py files - wait for better iteration
-- adapt java code to convert directly to netpyne
-- https://github.com/NeuroML/org.neuroml.export/blob/development/src/main/java/org/neuroml/export/neuron/NeuronWriter.java
-- https://github.com/NeuroML/org.neuroml.export/blob/development/src/main/java/org/neuroml/export/pynn/PyNNWriter.java
-- check param files hhtut izhitut updated so can padraig can modify
-- add load cell function in netpyne in neuroml format


- differences:
1) declarative
2) netpyne makes use of mod files, but neuroml requires detailed specification
3) netpyne provides conversion from abstract specifications -> instantiated network, and support for multicompartment, and
  subcellular connectivity
4) netpyne provides instantiation of NEURON objects, and parallel simulation of network

- options:
-- long-term: use Neuroml internally for instantiated network 
-- short-term:  Neuron-based and conversion to NeuroML; 


* 16jan27 Consolidate connectivity functions
** Full conn -> fullConn
- =probConn with prob = 1?
** combine convConn, divConn, and randConn?
- probFunc = optional param -- if not present, then:
- select function depending of param included? eg. 'convergence', 'divergence', 'probability'
** Random conn with N conns per postsyn cell (convergence) -> convConn
~= conv conn
- conv param allowed as function eg. uniform, gauss
- also allows for weight+delay as func
** Random conn with N conns per presyn cell (divergence) -> divConn
** Random conn with fixed, function (including distance-dep) prob, weight, delay -> probConn
*** fixed
number eg. 5
*** func
- uniform
*** func dist-dep
- function of yfrac
- function of 3d distance
- generalizable to func of 
-- (pre, post, dist) + '_' + (xfrac, yfrac, zfrac, x, y, z) = 18 variables
-- (dist) + '_' + (xznorm, xyznorm, xz, xyz) = 4 variables
-- total: 22 variables
- create dictionary with each variable associated to its value/variable/function -- don't evaluate to save
- enter as string, and convert to function
- other variables allowed: any that are defined in f.net.params[var] -- arbitrary
** implementation
- use **kwargs
- string based - converted to function, extract variables
- arguments weight and delay valid for all conn types; and can be functional

Options to implement func:
*** lambda with kwargs - NO
prob = lambda(**d): d['pre_ynorm']*0.5
*** lambda with args - NO!
prob = lambda(*d): d[3]*0.5
*** string
prob = 'pre_ynorm*0.5'
prob = 'post_xyznorm*0.5
prob = 'uniform()'
prob = 'gauss(10,5)'
prob = 'uniform(1,5)'
- functions allow all of python random functions 
- internal implementation:
-- identifiy variables (pre, post or dist x,y,z,2d or 3d; plus any arbitrary defined in netParams)
-- define dict mapping variable name to lambda function returning actual variable using preCell and postCell as args
-- convert string to lambda function with variables as arguments
-- call lambda function with args as dict with each entry mapped to lambda func that returns the variable
*** efficiency
- calculate all delay, prob, weight funcs together at the beginning
- calcualte weights+delays for all possible conns despite some will not be used?
- alternative might be more comput costly (calling 1 by 1), and repeated code
- use map(lambda, list)
- calculate final list of weights,probs,delays and pass to specific conn func

*** errors
- arguments by reference using lambda func only works for 'delay' if 'convergence' func loops over preCellsTags and postCells
  -- even though they have nothing to do with each other!! whats going on? python bug? Im missing something?!

** comparison to PyNN
*** class AllToAllConnector(allow_self_connections=True, safe=True, callback=None)[source]
=fullConn
*** class OneToOneConnector(safe=True, callback=None)[source]
=conn rule specifying cell ids
*** class FixedProbabilityConnector(p_connect, allow_self_connections=True, rng=None, safe=True, callback=None)[source]
=probConn
*** class FromListConnector(conn_list, column_names=None, safe=True, callback=None)[source]
=conn rule specifying cell ids
*** class FromFileConnector(file, distributed=False, safe=True, callback=None)[source]
=conn rule specifying cell ids
*** class ArrayConnector(array, safe=True, callback=None)[source]
=conn rule specifying cell ids
*** class FixedNumberPreConnector(n, allow_self_connections=True, with_replacement=False, rng=None, safe=True, callback=None)
=divConn
*** class DistanceDependentProbabilityConnector(d_expression, allow_self_connections=True, rng=None, safe=True, callback=None)[source]
= probConn with dist-dep func

*** class IndexBasedProbabilityConnector(index_expression, allow_self_connections=True, rng=None, safe=True, callback=None)[sour
= conn rule specifying cell ids
*** class SmallWorldConnector(degree, rewiring, allow_self_connections=True, n_connections=None, rng=None, safe=True, callback=N
not implemented
*** class CSAConnector(cset, safe=True, callback=None)[source]
not implemented
** comparison to NEST
*** ConvergentConnect
Connect many source nodes to one target node.
=convConn
*** DivergentConnect
Connect one source node to many target nodes.
= divConn
*** RandomConvergentConnect
Randomly connect one source node to many target nodes.
= convConn with uniform func
*** RandomDivergentConnect
Randomly connect one source node to many target nodes.
= divConn with uniform func
*** BinomialConvergentConnect
Connect a target to a binomial number of sources.
??


* 16feb18 Changing folder structure and adding setup.py
** file structure
- root
-- examples/: param and run files
-- netpyne/: actual package
-- doc/: documentation
-- setup.py: required for pip install
-- other root files
** OK how to add $PYTHONPATH in setup.py?
** OK Make sure all examples and tuts work
- examples - ok
- tutorial examples - ok
- claus - ok
- M1Network -ok
** OK Update documentation
** OK Update README

* 16feb25 Adding STDP and RL
** old stdp code
if s.usestdp and ([s.cellpops[pregid],s.cellpops[pstgid]] in s.plastConns): # If using STDP and these pops are set to be plastic connections
            if sum(abs(s.stdprates[s.EorI[pregid],:]))>0 or sum(abs(s.RLrates[s.EorI[pregid],:]))>0: # Don't create an STDP connection if the learning rates are zero
                for r in range(s.nreceptors): # Need a different STDP instances for each receptor
                    if newcon.weight[r]>0: # Only make them for nonzero connections
                        stdpmech = h.STDP(0,sec=s.dummies[pstid]) # Create STDP adjuster
                        stdpmech.hebbwt = s.stdprates[s.EorI[pregid],0] # Potentiation rate
                        stdpmech.antiwt = s.stdprates[s.EorI[pregid],1] # Depression rate
                        stdpmech.wmax = s.maxweight # Maximum synaptic weight
                        precon = s.pc.gid_connect(pregid,stdpmech); precon.weight[0] = 1 # Send presynaptic spikes to the STDP adjuster
                        pstcon = s.pc.gid_connect(pstgid,stdpmech); pstcon.weight[0] = -1 # Send postsynaptic spikes to the STDP adjuster
                        h.setpointer(s.connlist[-1]._ref_weight[r],'synweight',stdpmech) # Associate the STDP adjuster with this weight
                        s.stdpmechs.append(stdpmech) # Save STDP adjuster
                        s.precons.append(precon) # Save presynaptic spike source
                        s.pstcons.append(pstcon) # Save postsynaptic spike source
                        s.stdpconndata.append([pregid,pstgid,r]) # Store presynaptic cell ID, postsynaptic, and receptor
                        if s.useRL: # using RL
                            stdpmech.RLon = 1 # make sure RL is on
                            stdpmech.RLhebbwt = s.RLrates[s.EorI[pregid],0] # Potentiation rate
                            stdpmech.RLantiwt = s.RLrates[s.EorI[pregid],1] # Depression rate
                            stdpmech.tauhebb = stdpmech.tauanti = s.stdpwin # stdp time constant(ms)
                            stdpmech.RLwindhebb = stdpmech.RLwindhebb = s.eligwin # RL eligibility trace window length (ms)
                            stdpmech.useRLexp = s.useRLexp # RL 
                            stdpmech.softthresh = s.useRLsoft # RL soft-thresholding
                        else:
                            stdpmech.RLon = 0 # make sure RL is off
** how to update at intervals
*** why needed?
- periodic saves (eg. of weights)
- update virtual arm apparatus
- run RL on syns based on critic signal
- calculate LFP

*** option 1: run sim for short periods of time (not efficient?)
while round(h.t) < s.duration:
        s.pc.psolve(min(s.duration,h.t+s.loopstep)
*** option 2: finitialize handler?
** netpyne format
- within conn rule

- One set of STDP params for each conn rule:
STDPparams = {'hebbwt': , 'antiwt':, 'wmax':, 'RLon': , 'RLhebbwt', 'RLantiwt', 'tauhebb', 'RLwindhebb', 'useRLexp', 'softthresh'

  stdpmech.RLon = 1 # make sure RL is on
                            stdpmech.RLhebbwt = s.RLrates[s.EorI[pregid],0] # Potentiation rate
                            stdpmech.RLantiwt = s.RLrates[s.EorI[pregid],1] # Depression rate
                            stdpmech.tauhebb = stdpmech.tauanti = s.stdpwin # stdp time constant(ms)
                            stdpmech.RLwindhebb = stdpmech.RLwindhebb = s.eligwin # RL eligibility trace window length (ms)
                            stdpmech.useRLexp = s.useRLexp # RL 
                            stdpmech.softthresh = s.useRLsoft # RL soft-thresholding
'plasticity': {'mech': 'STDP', 'params': STDPparams}

- Single set of STDP params for all:
'plasticity':  'STDP'

* 16mar04 Adding subcellular synaptic conn distribution
** sectionList
- need during runtim? for connectivity purposes?
- option 1: can include section attribute named 'sectionList' and add name, but could take longer to set properties if have to search all
- option 2: can include as dict in sectionLists in cellProp, and then add all sections inside of each sectionList dict; but would
  change the consistency of the generic implementation (cell.py etc)
- option 3: can add additional dict sectionLitst in cellProp, simply including list of sections (need to return additional
  dic from util.py function)
- best options seem 1 or 3
- Need to create sectionList at runtime? useful?

-- iterating over sectionList
from neuron import h
​
s = h.Section(name='s')
a = h.Section(name='a')
d = h.Section(name='d')
​
sl1 = h.SectionList()
sl1.append(sec=s)
​
sl2 = h.SectionList()
sl2.append(sec=a)
sl2.append(sec=d)
​
section_lists = h.List('SectionList')
print 'There are %d SectionLists' % section_lists.count()
​
for i in xrange(int(section_lists.count())):
    print 'SectionList %d has the following sections: %r' % (i + 1, [str(sec.hname()) for sec in section_lists.o(i)])

--- Note that this iterates over all SectionList objects, not just those associated with a particular cell.
--- Alternatively, itertools.chain.from_iterable
--- https://docs.python.org/2/library/itertools.html#itertools.chain.from_iterable

*** NeuroML implements as option 3:
 <segmentGroup id="prox_dend_soma">
                <include segmentGroup="comp_1"/>
                <include segmentGroup="comp_41"/>
                <include segmentGroup="comp_28"/>
                <include segmentGroup="comp_15"/>
                <include segmentGroup="comp_2"/>
            </segmentGroup>

*** Netpyne implementation
- Option 3
cellRule = {'label': 'PYRrule', 'conditions': {'cellType': 'PYR'},  'sections': {}, 'sectionLists': {}} 
...
cellRule['sectionLists']['all'] = ['soma', 'dend']

- Can't define properties of section lists in netpyne
- Neuron section list object not created - just python version
- Can import cell with sectionlists
- Can use sectionlists in connectivity

* Netpyne Models/Users
** Github examples folder
** Documentation tutorials
** M1 network (Salva, Neurosim)
** Parallel Neuron paper (Alex, Neurosim)
** Claustrum network (Jing, George Augustine)
** Sensorimotor network + arm (Cliff, Kerr lab)


* TODO
** DONE - unify cellmodel 
- cell prop conditions: 
-- cellType=RS; cellModel=HH
-- cellType=RS; cellModel=Izhi
- provide option to insert distribute mechanism or point process
- single cell class 
- then add support for 2007a - spikes+record from izh._ref_V

- INCLUDE BOTH HH AND IZHI IN SAME RULE AND USE CELLMODEL TAG IN CELL TO CHOOSE WHICH TO USE!!

** DONE - Overwrite files optional (timestamp)
** DONE - Default simConfig
** DONE - Import cells with generic set of arguments
** DONE - Use mechanismType to make importCell more robust
https://www.neuron.yale.edu/neuron/static/new_doc/modelspec/programmatic/mechtype.html
https://www.neuron.yale.edu/neuron/static/new_doc/programming/mechstan.html
http://neurosimlab.org/ramcd/pyhelp/mechstan.html#MechanismStandard - 2nd example

- use params with initial underscore '_var' to indicate used by netpyne, not mechanism/pointprocess variable - add to docu!
** DONE Check Izhi2007a
** DONE Distinguish syns from other point processes 
- no way to do it
- assume 'syn' in name -- establish naming convention -- added to doc

- syn: true, false, false (.is_netcon_target() = True; .has_net_event(i) = False; .is_artificial() = false)
- artcell: true, true?, true
- izh2007a: true, true, false
- izhi2007b: true, false, false

MechanismType.is_netcon_target()
Syntax:
boolean =  mt.is_netcon_target(i)
Description:
The i'th point process has a NET_RECEIVE block and can therefore be a target for a NetCon object.

MechanismType.has_net_event() 
Syntax:
boolean = mt.has_net_event(i)
Description:
The i'th point process has a net_event call in its NET_RECEIVE block and can therefore be a source for a NetCon object. This
means it is NetCon stimulator or that the point process can be used as an artificial neural network cell.

MechanismType.is_artificial() 
Syntax:
boolean = mt.is_artificial(i)
Description:
The i'th point process is an ARTIFICIAL_CELL and can therefore be a source for a NetCon object. This means it is NetCon stimulator or that the point process can be used as an artificial neural network cell.

This seems to have, but does not, equivalent functionality to has_net_event() and was introduced because ARTIFICIAL_CELL
objects are no longer located in sections. Some ARTIFICIAL_CELLs such as the PatternStim do not make use of net_event in
their implementation, and some PointProcesses do use net_event and must be located in sections for their proper function,
e.g. reciprocal synapses.

** DONE Make list of data saved part of simCfg
- simConfig, netParams, net, simData
- added to docu

** DONE Test importing Friesen, mainen, traub cell models
- Read python names in importCell
-- use dir; check if section; compare with secList

- 'synReceptor' check for syn type not name

** DONE Associate_gid from axon
- section['spikeGenLoc'] = 0.5

** DONE HDF5
- tried h5py (http://docs.h5py.org/en/latest/quick.html)
Saving output as example-20160108_200403.hdf5... 
Traceback (most recent call last):
  File "init.py", line 24, in <module>
    f.sim.saveData()                    # save params, cell info and sim output to file (pickle,mat,txt,etc)
  File "/usr/site/nrniv/pypkg/netpyne/sim.py", line 346, in saveData
    hickle.dump(dataSave, f.cfg['filename']+'.hdf5', mode='w')
  File "/u/salvadord/anaconda/lib/python2.7/site-packages/hickle.py", line 376, in dump
    dumper(obj, h5f, **kwargs)
  File "/u/salvadord/anaconda/lib/python2.7/site-packages/hickle.py", line 300, in dump_dict
    _dump_dict(obj, hgroup, **kwargs)
  File "/u/salvadord/anaconda/lib/python2.7/site-packages/hickle.py", line 282, in _dump_dict
    _dump_dict(dd[key], new_group, **kwargs)
  File "/u/salvadord/anaconda/lib/python2.7/site-packages/hickle.py", line 269, in _dump_dict
    hgroup.create_dataset("%s" % key, data=dd[key], **kwargs)
  File "/u/salvadord/anaconda/lib/python2.7/site-packages/hickle.py", line 88, in create_dataset
    return super(H5GroupWrapper, self).create_dataset(*args, **kwargs)
  File "/u/salvadord/anaconda/lib/python2.7/site-packages/h5py/_hl/group.py", line 94, in create_dataset
    dsid = dataset.make_new_dset(self, shape, dtype, data, **kwds)
  File "/u/salvadord/anaconda/lib/python2.7/site-packages/h5py/_hl/dataset.py", line 79, in make_new_dset
    tid = h5t.py_create(dtype, logical=1)
  File "h5t.pyx", line 1389, in h5py.h5t.py_create (h5py/h5t.c:13046)
  File "h5t.pyx", line 1463, in h5py.h5t.py_create (h5py/h5t.c:12893)
TypeError: Object dtype dtype('O') has no native HDF5 equivalent

- tried hdf5storage (http://pythonhosted.org/hdf5storage/introduction.html#getting-started)
Traceback (most recent call last):
  File "init.py", line 24, in <module>
    f.sim.saveData()                    # save params, cell info and sim output to file (pickle,mat,txt,etc)
  File "/usr/site/nrniv/pypkg/netpyne/sim.py", line 348, in saveData
    hdf5storage.write(dataSave, filename=f.cfg['filename']+'.hdf5')
  File "build/bdist.macosx-10.5-x86_64/egg/hdf5storage/__init__.py", line 1399, in write
    
  File "build/bdist.macosx-10.5-x86_64/egg/hdf5storage/__init__.py", line 1318, in writes
    
  File "build/bdist.macosx-10.5-x86_64/egg/hdf5storage/lowlevel.py", line 114, in write_data
  File "build/bdist.macosx-10.5-x86_64/egg/hdf5storage/Marshallers.py", line 1557, in write
NotImplementedError: Dictionaries with non-unicode keys are not supported: 'netParams'

- try this: http://stackoverflow.com/questions/16705274/fastest-way-to-convert-a-dicts-keys-values-from-str-to-unicode

- converted to unicode using dict2utf8() func, but now get segmentation fault for all except simConfig dict

- empty dicts {} caused seg fault -- replaced with []

** DONE Enable providing num spikes in Netstim (paper)
** DONE Set v_init before calling init() (paper)
- netParam param
- 'vinit' param in section
- automatically set vinit=vr for izhis - not needed cause fixed mod
- add to doc

** DONE Conv and div conn (paper)
- made conn very flexible by allowing function for: prob, conv, div, weight and delay
- funcs can include spatial/distance variables; and others defined in netParams
- can implement a large range of functionalities with very generic implementation
** DONE Add x,z,xnorm,znorm positions (paper)
** DONE Plot cell from specific population (paper)
- and or one of each population

** DONE Allow to define xrange and zrange of pops
- also xfracrange and zfracrange
- for fixed cell density, easy, just don't calcualte rand
- for functional density more complex
- make generic so no distinction between coordinates
** DONE Fix NetStim connectivity
- no convergent or divergent; only full and prob
** DONE fix so lambda funcs saved as string
** DONE setup.py pip install (paper)
** DONE timing (paper)
** DONE Record subset of time, and only record traces for subset of cells
- recordTraces = ['all', 'P', 1]
- plotTraces = ['all', 'P', 1]

- pops means just 1 cell of that pop!
- automatically add elements of plotTraces to recordTraces!!

** DONE Fix pops starting with same characters
EMstim == EM !!

** Cell position in ellipsoid 
** Add support for SectionLists (paper)

** simple LFP (paper) 
- simple voltage sum 
** Plot conn matrix (paper)
- by cells and populations
** Subcellular synaptic connectivity (paper)
- distribution of syns across segments based on empirical map
- concept of logical connection subsumes subset of unitary connections
** import/export NeuroML (paper)
- padraig (see notes above)
** Functions to load simConfig, netParams, net, simData (paper)
** importCell can modify h variables (eg. celsius) (paper)
- maybe just note in documentation that shouldnt modify h.vars - undesired effects



** Plasticity (paper?)
- Hebbian
- STDP

** Stimulation - iclamp,vclamp, opto, microelectrode (paper?)
- maybe iclamp for paper?
- can implement via pointp in cell properties


** User input error checking
eg. getattr(a,b, DEFAULT VALUE!)
** Travis CI
test automatically examples
** Evol params batch
** Set positions in Neuron's 3D space
** LFPy
cliff?
** Save plots to file
** Conn/props just in master node
** normalized transfer entropy (nTE)
cliff?
** Granger causality
cliff?
** Add support for point neuron outside of section
eg. IntFire1,2,3,4 
 ARTIFICIAL_CELL means
     3 	: that this model not only has a NET_RECEIVE block and does NOT
     4 	: have a BREAKPOINT but is not associated with a
     5 	: section location or numerical integrator. i.e it does not
     6 	: refer to v or any ions or have a POINTER. It is entirely isolated
     7 	: and depends on discrete events from the outside to affect it and
     8 	: affects the outside only by sending discrete events.

- outside of section so need different method - maybe not worth? 

** RxD 
/u/samn/m1dyst -> models/m1dyst
https://www.neuron.yale.edu/neuron/static/docs/rxd/index.html
** Saving at intervals
** modify instantiated network
- load net and provide new set of params
** Make simulator-indep using NeuroML internal format
** GUI
** Test adaptive exp cell (no Neuron code)
student doing
** Make opensource with contributions from community
There are several ways to contribute to PyNN:

reporting bugs, errors and other mistakes in the code or documentation;
making suggestions for improvements;
fixing bugs and other mistakes;
adding or maintaining a simulator backend;
major refactoring to improve performance, reduce code complexity, or both.
